NAME: Atibhav Mittal
EMAIL: atibhav.mittal6@gmail.com
ID: 804598987

2.3.1
In the 1 or 2 thread list, since any lock that needs to be acquired can be almost immediately acquired, most of the cycles go towards performing the operations. These are the most expensive operations because the insert, lookup and length, all are O(n) operations. Comparing these to the synchronization operations, which run almost in constant time, the list operations are more expensive.
In the case with larger number of threads, it depends on the mechanism of synchronization that we're using. For spin locks, most of the cycles go towards spinning, and trying to acquire the lock, since only 1 thread can perform an operation at a time. For mutex, we can assume that most of the cycles go toward performing operations. This is a fair assumption since once 
a thread obtains a lock, it can operate on the list. But if it doesn't then it goes to sleep and doesn't use up any CPU cycles.

2.3.2
The while loop that implements the spinning consumes the most lines of code. 
Since there are a large number of threads, and a single lock, each thread tries to acquire the lock. Since most threads can't acquire the lock, they continue spinning and hence spend a lot of CPU cycles performing this operation, making this operation extremely expensive.

2.3.3
The average wait time per operation rises dramatically with the number of threads because its measuring the CPU time. For each thread, it measures the CPU time individually and then sums it up, leading to an extremely large number. For the Completion time per operation, we measure it using the wall time and hence, it doesn't rise as dramatically with increasing number of threads. Hence if multiple threads are trying to acquire the lock, its going to
count that time multiple times when we measure the average wait time. However, 
for the completion time per operation we only measure it once, leading to a 
less dramatic increase as the number of threads increase. 

2.3.4
We get an improved throughput when we increase the number of lists. This is because of the fact that when we operate with a large number of threads, these
threads are not trying to acquire the same lock now. Since they work on sublists, they try to acquire the lock corresponding to that sublist and hence
spend less time trying to acquire locks/spinning and more time performing operations leading to higher throughput.
The throughput will not continue increasing as we increase the number of lists.
This is because of that fact that once we have more lists than threads, then it
is expected that each thread can easily acquire a lock to a particular sublist,
and operate on it (assuming equal distribution from the hash function). Hence,
in that case, it would be similar to running each thread independently. Since,
this is the optimal case, the throughput cannot be increased beyond this level.
The throughput of an N-way partitioned list is not equivalent to a single list
with fewer (1/N) threads. In the graphs attached, the performance of 8 lists with 4 threads is much better than the performance of a single list with 2 threads. Hence, this statement is not true from the graphs.
A possible reason for this is that with more lists, we have more resources and
hence the threads will likely try to acquire different locks. This leads to more time spent performing the actual operations and an increased throughput.

Notes for the Lab:
I used a hashing function for strings that I found on stack overflow. The link
is documented in the code, and here also:
https://stackoverflow.com/questions/7666509/hash-function-for-string
The reason I looked up a hash function instead of coming up with my own is because I wanted a more even distribution of keys to indices. Coming up with such a hash function on my own would take a lot of time, and seems unnecessary
for the purpose of the lab.

Files Included in the Tarball:

README: Contains answers to the questions in the lab and descriptions of the files in the tarball.

Makefile: Supports the GNU make command for this project. The targets include the default (executable) lab2_list, graphs (to generate graphs as described in the spec), profile (generate profile.out described below), dist (creates the
tarball), clean (removes all the files generated by the Makefile), and tests (runs the tests to generate graphs).

lab2b_list.gp: The data reduction script that converts the data from CSV files
to the plots described below (gnuplot script).

lab2b_list.csv: Contains the output report of all the testcases required to generate the graphs.

profile.out: Result of running pprof with the lab2_list program with a large number of threads and while using spin locks. This shows us where most of the 
CPU cycles are spent. 

SortedList.h: Header file defining the interface of the Linked List (doubly linked list with a dummy node)

SortedList.c: Contains implementation of the linked list.

lab2_list.c: Source code for the driver program that implements the "metadata" for the linked list such as which synchronization mechanism to use, how many threads to use, how many lists to use and how many iterations to perform.

lab2b_1.png: Graph showing the throughput for spin lock and Mutex using a single list as the number of threads increase.

lab2b_2.png: Graph showing the time spent per operation and the time spent waiting to acquire locks as the number of threads increases.

lab2b_3.png: Graph showing successful iterations using spin lock and mutex and the number of threads. (Basically shows that using these mechanisms, our list
becomes thread-safe).

lab2b_4.png: Shows the throughput for different number of sub-lists as the number of threads increases, while using Mutex locks.

lab2b_5.png: Shows the throughput for different number of sub-lists as the 
number of threads increases, using spin locks as the synchronization mechanism.







